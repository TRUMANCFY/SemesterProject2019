{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-document Summarization using Tensor Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Introduction\n",
    "This notebook is a recurrance of the method of test labelling mentioned in the paper [http://www.scielo.org.mx/pdf/cys/v18n3/v18n3a12.pdf]\n",
    "\n",
    "There is mainly following steps mentioned in this notebookï¼š\n",
    "- data extraction and pre-processing\n",
    "- tensor construction\n",
    "- tensor decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. data extraction and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['computer', 'sport', 'politics', 'entertainment', 'science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('data')\n",
    "for t in topics:\n",
    "    os.makedirs(os.path.join('data', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wikiapi\n",
    "import wikipedia as wiki\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api = wikiapi.Wikipedia('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment\n",
      "science\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "for t in topics:\n",
    "    print(t)\n",
    "    tag = True\n",
    "    while tag:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            topic_dict[t] = wiki.search(t)\n",
    "            tag = False\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entertainment\n",
      "Entertainment!\n",
      "YG Entertainment\n",
      "JYP Entertainment\n",
      "SM Entertainment\n",
      "WWE\n",
      "Yuehua Entertainment\n",
      "Coridel Entertainment\n",
      "Sony Entertainment\n",
      "InXile Entertainment\n",
      "Science\n",
      "Science (disambiguation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caifengyu/anaconda3/lib/python3.6/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/caifengyu/anaconda3/lib/python3.6/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural science\n",
      "Science fiction\n",
      "Social science\n",
      "Forensic science\n",
      "Political science\n",
      "Rocket science\n",
      "Branches of science\n",
      "Computer science\n"
     ]
    }
   ],
   "source": [
    "for tk, tv in topic_dict.items():\n",
    "    for pageName in tv:\n",
    "        print(pageName)\n",
    "        tag = True\n",
    "        cnt = 0\n",
    "        while tag:\n",
    "            try:\n",
    "                pageTmp = wiki.page(pageName)\n",
    "                pageContent = pageTmp.content\n",
    "                with open(os.path.join(os.path.join('data', tk), pageName + '.txt'), 'w') as w:\n",
    "                    w.write(pageContent)\n",
    "                tag = False\n",
    "            except:\n",
    "                cnt += 1\n",
    "                if cnt < 10:\n",
    "                    continue\n",
    "                else:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will preprocess our data:\n",
    "- split the sentence\n",
    "- remove the stopwords\n",
    "- word stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Split the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSplit(inputContent):\n",
    "    res = nltk.sent_tokenize(inputContent)\n",
    "    res_filter = [i for i in res if len(i) > 10]\n",
    "    return res_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "for tp in topics:\n",
    "    fileNames = os.listdir(os.path.join('data/', tp))\n",
    "    for fileName in fileNames:\n",
    "        with open(os.path.join(os.path.join('data', tp), fileName)) as f:\n",
    "            text_dict[fileName] = sentenceSplit(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Remove the stop words and word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceRemoveStopwords(inputSentence):\n",
    "    \"\"\"\n",
    "        input: the string of input\n",
    "        output: list of stemmed tokens\n",
    "    \"\"\"\n",
    "    # lowercase\n",
    "    inputLower = inputSentence.lower()\n",
    "    # get the word and remove the punctuation\n",
    "    regTokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_filter = [t for t in regTokenizer.tokenize(inputLower) if t not in stop_words]\n",
    "    # stem the word\n",
    "    tokens_stemmer = [stemmer.stem(t) for t in tokens_filter]\n",
    "    return tokens_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens = defaultdict(list)\n",
    "for fileName, text in text_dict.items():\n",
    "    for sen in text:\n",
    "        wordTokens[fileName].append(sentenceRemoveStopwords(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Topic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the complete textual data set\n",
    "text_complete = [[word for sent in sents for word in sent] for sents in wordTokens.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word = [dictionary.doc2bow(text) for text in text_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPIC = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=bag_of_word, num_topics=NUM_TOPIC, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"sport\" + 0.016*\"wwe\" + 0.008*\"wrestl\" + 0.007*\"player\" + 0.006*\"squash\"')\n",
      "(1, '0.019*\"secur\" + 0.012*\"system\" + 0.009*\"comput\" + 0.009*\"attack\" + 0.006*\"network\"')\n",
      "(2, '0.032*\"entertain\" + 0.007*\"form\" + 0.007*\"music\" + 0.007*\"perform\" + 0.006*\"marvel\"')\n",
      "(3, '0.020*\"scienc\" + 0.020*\"comput\" + 0.011*\"polit\" + 0.009*\"use\" + 0.006*\"studi\"')\n",
      "(4, '0.016*\"network\" + 0.014*\"comput\" + 0.013*\"sport\" + 0.012*\"graphic\" + 0.011*\"use\"')\n"
     ]
    }
   ],
   "source": [
    "topic_select = ldamodel.print_topics(num_words=5)\n",
    "for tp in topic_select:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_complete = [sent for sents in wordTokens.values() for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_sent = corpora.Dictionary(sent_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_sent = [dictionary_sent.doc2bow(text) for text in sent_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPIC = 5\n",
    "ldamodel_sent = gensim.models.ldamodel.LdaModel(corpus=bag_of_word_sent, num_topics=NUM_TOPIC, id2word=dictionary_sent, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"network\" + 0.010*\"use\" + 0.010*\"marvel\" + 0.009*\"system\" + 0.009*\"secur\"'),\n",
       " (1,\n",
       "  '0.031*\"entertain\" + 0.031*\"comput\" + 0.012*\"use\" + 0.011*\"program\" + 0.008*\"perform\"'),\n",
       " (2,\n",
       "  '0.045*\"scienc\" + 0.016*\"social\" + 0.013*\"natur\" + 0.012*\"polit\" + 0.011*\"studi\"'),\n",
       " (3,\n",
       "  '0.009*\"wwe\" + 0.008*\"film\" + 0.007*\"game\" + 0.007*\"sport\" + 0.006*\"world\"'),\n",
       " (4,\n",
       "  '0.015*\"sport\" + 0.009*\"state\" + 0.007*\"court\" + 0.006*\"new\" + 0.006*\"unit\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_select_sent = ldamodel_sent.print_topics(num_words=5)\n",
    "topic_select_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Tensor Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 build up the topic-term mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10493"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_terms = len(dictionary.id2token.keys())\n",
    "num_of_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_topics = len(topic_select_sent)\n",
    "num_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_doc = len(text_complete)\n",
    "num_of_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the map between topics and terms\n",
    "topic_term_mapping = np.zeros((num_of_terms, num_of_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1a1816fc50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_tp = 0.5\n",
    "for sent in sent_complete:\n",
    "    word_inds = dictionary_sent.doc2bow(sent)\n",
    "    topic_prob = ldamodel_sent.get_document_topics(word_inds)\n",
    "    for tp_prob in topic_prob:\n",
    "        if tp_prob[1] > thresh_tp:\n",
    "            for word_ind in word_inds:\n",
    "                topic_term_mapping[word_ind[0], tp_prob[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52465"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the total number of entry in topic_term_mapping\n",
    "num_of_terms * num_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17358.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the valid entry of topic_term_mapping\n",
    "np.sum(topic_term_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 build up tf-idf mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfModel = TfidfModel(dictionary=dictionary, corpus=bag_of_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 passage index to sentence index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_ind_to_sent_ind = lambda x: dictionary_sent.token2id[dictionary.id2token[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ind_to_passage_ind = lambda x: dictionary.token2id[dictionary_sent.id2token[x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 tensor construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTFIDF(idxList):\n",
    "    res = np.zeros((num_of_terms))\n",
    "    for idx, val in idxList:\n",
    "        res[idx] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_res = np.zeros((num_of_doc, num_of_topics, num_of_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_mapping_transfer = np.zeros((num_of_terms, num_of_topics))\n",
    "for idx_term in range(num_of_terms):\n",
    "    transfered_ind = sent_ind_to_passage_ind(idx_term)\n",
    "    for idx_topic in range(num_of_topics):\n",
    "        if topic_term_mapping[transfered_ind, idx_topic]:\n",
    "            topic_term_mapping_transfer[idx_term, idx_topic] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_text in range(len(text_complete)):\n",
    "    tfidfValue = tfidfModel[dictionary.doc2bow(text_complete[idx_text])]\n",
    "    tfidfList = fillTFIDF(tfidfValue)\n",
    "    for idx_topic in range(len(topic_select_sent)):\n",
    "        tensor_res[idx_text, idx_topic, :] = topic_term_mapping_transfer[:, idx_topic] * tfidfList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Tensor Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorly.decomposition import tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5, 10493)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84656"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tensor.pickle', 'wb') as f:\n",
    "    pickle.dump(tensor_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('tensor_new.mat', {'t_new': tensor_res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorly.decomposition import tucker\n",
    "with open('tensor.pickle', 'rb') as f:\n",
    "    tensor_res = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 44, 10493)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = np.sum(tensor_res, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 44)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 0, 4, 0, 4, 3, 4, 0, 1, 1, 0, 1, 1,\n",
       "       0, 2, 2, 2, 1, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 3, 2, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(topic_terms, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_result = tucker(tensor_res, ranks=[44,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(decompose_result[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decompose_result[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10493, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decompose_result[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_res = np.reshape(decompose_result[1][0], (5,))\n",
    "doc_res = np.reshape(decompose_result[1][1], (44,))\n",
    "term_res = np.reshape(decompose_result[1][2], (10493,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc_mapping = np.outer(doc_res, topic_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(doc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(topic_doc_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83284"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction error=0.8108888835123509, variation=0.009696198435317216.\n",
      "reconstruction error=0.8059386639604648, variation=0.004950219551886126.\n",
      "reconstruction error=0.8036228595973266, variation=0.002315804363138163.\n",
      "reconstruction error=0.8023948047615325, variation=0.0012280548357941345.\n",
      "reconstruction error=0.8015184195180648, variation=0.00087638524346767.\n",
      "reconstruction error=0.8007554250441414, variation=0.0007629944739233618.\n",
      "reconstruction error=0.8000306285496557, variation=0.0007247964944857577.\n",
      "reconstruction error=0.7993227869028, variation=0.000707841646855667.\n",
      "reconstruction error=0.7986300170785783, variation=0.0006927698242217284.\n",
      "reconstruction error=0.7979554557218205, variation=0.0006745613567578079.\n",
      "reconstruction error=0.7973006858142758, variation=0.00065476990754465.\n",
      "reconstruction error=0.7966644273161386, variation=0.0006362584981371988.\n",
      "reconstruction error=0.7960447587199816, variation=0.0006196685961570303.\n",
      "reconstruction error=0.795442627601974, variation=0.0006021311180075539.\n",
      "reconstruction error=0.7948646730960971, variation=0.0005779545058769431.\n",
      "reconstruction error=0.7943238642177126, variation=0.00054080887838448.\n",
      "reconstruction error=0.7938369593364366, variation=0.00048690488127600684.\n",
      "reconstruction error=0.7934191397620701, variation=0.00041781957436648476.\n",
      "reconstruction error=0.7930783569300013, variation=0.0003407828320688422.\n",
      "reconstruction error=0.7928128034104186, variation=0.00026555351958268414.\n",
      "reconstruction error=0.792612738668767, variation=0.00020006474165157329.\n",
      "reconstruction error=0.7924647334881624, variation=0.00014800518060464896.\n",
      "reconstruction error=0.7923555299561378, variation=0.00010920353202459321.\n",
      "reconstruction error=0.7922741401288838, variation=8.138982725403032e-05.\n",
      "reconstruction error=0.7922123545922783, variation=6.178553660540764e-05.\n",
      "reconstruction error=0.7921643944916398, variation=4.7960100638566594e-05.\n",
      "reconstruction error=0.792126305285536, variation=3.8089206103819606e-05.\n",
      "reconstruction error=0.7920953946671726, variation=3.0910618363355624e-05.\n",
      "reconstruction error=0.7920698086467134, variation=2.5586020459211767e-05.\n",
      "reconstruction error=0.792048242571021, variation=2.156607569236968e-05.\n",
      "reconstruction error=0.792029754252666, variation=1.8488318355047717e-05.\n",
      "reconstruction error=0.792013645900927, variation=1.6108351738930438e-05.\n",
      "reconstruction error=0.7919993896290644, variation=1.4256271862689296e-05.\n",
      "reconstruction error=0.7919865796909947, variation=1.2809938069624494e-05.\n",
      "reconstruction error=0.7919749009367945, variation=1.1678754200272756e-05.\n",
      "reconstruction error=0.7919641071796696, variation=1.0793757124871739e-05.\n",
      "reconstruction error=0.7919540057621764, variation=1.0101417493135756e-05.\n",
      "reconstruction error=0.7919444461433588, variation=9.559618817656101e-06.\n",
      "reconstruction error=0.7919353112114998, variation=9.134931858945095e-06.\n",
      "reconstruction error=0.7919265105293763, variation=8.800682123499826e-06.\n",
      "reconstruction error=0.7919179750049075, variation=8.535524468888944e-06.\n",
      "reconstruction error=0.791909652645879, variation=8.322359028478665e-06.\n",
      "reconstruction error=0.7919015051569009, variation=8.14748897803863e-06.\n",
      "reconstruction error=0.7918935051995097, variation=7.999957391202628e-06.\n",
      "reconstruction error=0.7918856341784108, variation=7.871021098893038e-06.\n",
      "reconstruction error=0.7918778804467763, variation=7.75373163452997e-06.\n",
      "reconstruction error=0.7918702378459268, variation=7.642600849511183e-06.\n",
      "reconstruction error=0.7918627045121166, variation=7.533333810205711e-06.\n",
      "reconstruction error=0.7918552818969635, variation=7.422615153074297e-06.\n",
      "reconstruction error=0.7918479739592101, variation=7.307937753386895e-06.\n",
      "reconstruction error=0.7918407864944602, variation=7.187464749924821e-06.\n",
      "reconstruction error=0.7918337265768117, variation=7.059917648488678e-06.\n",
      "reconstruction error=0.7918268020921135, variation=6.924484698189737e-06.\n",
      "reconstruction error=0.791820021347199, variation=6.78074491455849e-06.\n",
      "reconstruction error=0.7918133927430914, variation=6.6286041076057955e-06.\n",
      "reconstruction error=0.7918069245029604, variation=6.468240130952374e-06.\n",
      "reconstruction error=0.791800624447752, variation=6.30005520840804e-06.\n",
      "reconstruction error=0.791794499813948, variation=6.124633804005519e-06.\n",
      "reconstruction error=0.7917885571090308, variation=5.942704917161201e-06.\n",
      "reconstruction error=0.7917828020009748, variation=5.755108056004765e-06.\n",
      "reconstruction error=0.7917772392385495, variation=5.56276242535958e-06.\n",
      "reconstruction error=0.7917718725995145, variation=5.366639034942722e-06.\n",
      "reconstruction error=0.7917667048638932, variation=5.167735621314229e-06.\n",
      "reconstruction error=0.7917617378095922, variation=4.9670543009749935e-06.\n",
      "reconstruction error=0.7917569722276194, variation=4.765581972820954e-06.\n",
      "reconstruction error=0.7917524079541729, variation=4.564273446527878e-06.\n",
      "reconstruction error=0.7917480439168727, variation=4.36403730019741e-06.\n",
      "reconstruction error=0.7917438781924708, variation=4.1657244018722395e-06.\n",
      "reconstruction error=0.7917399080734496, variation=3.97011902120159e-06.\n",
      "reconstruction error=0.7917361301410683, variation=3.777932381376914e-06.\n",
      "reconstruction error=0.7917325403425736, variation=3.5897984946853256e-06.\n",
      "reconstruction error=0.791729134070506, variation=3.4062720675187563e-06.\n",
      "reconstruction error=0.79172590624227, variation=3.2278282360298505e-06.\n",
      "reconstruction error=0.7917228513783817, variation=3.054863888296566e-06.\n",
      "reconstruction error=0.7917199636780663, variation=2.8877003154237357e-06.\n",
      "reconstruction error=0.791717237091133, variation=2.726586933343711e-06.\n",
      "reconstruction error=0.7917146653853117, variation=2.5717058212970656e-06.\n",
      "reconstruction error=0.7917122422084397, variation=2.423176871935162e-06.\n",
      "reconstruction error=0.7917099611451257, variation=2.281063314013565e-06.\n",
      "reconstruction error=0.7917078157676799, variation=2.145377445805785e-06.\n",
      "reconstruction error=0.7917057996812624, variation=2.016086417477858e-06.\n",
      "reconstruction error=0.7917039065633441, variation=1.8931179183168112e-06.\n",
      "reconstruction error=0.7917021301976593, variation=1.7763656847691323e-06.\n",
      "reconstruction error=0.7917004645029322, variation=1.6656947271487255e-06.\n",
      "reconstruction error=0.7916989035567031, variation=1.560946229051119e-06.\n",
      "reconstruction error=0.7916974416146312, variation=1.4619420719563792e-06.\n",
      "reconstruction error=0.7916960731256645, variation=1.3684889667020528e-06.\n",
      "reconstruction error=0.7916947927434899, variation=1.280382174617678e-06.\n",
      "reconstruction error=0.791693595334662, variation=1.1974088278687844e-06.\n",
      "reconstruction error=0.7916924759838027, variation=1.119350859335455e-06.\n",
      "reconstruction error=0.7916914299962561, variation=1.045987546577365e-06.\n",
      "reconstruction error=0.7916904528985426, variation=9.77097713517061e-07.\n",
      "reconstruction error=0.7916895404369452, variation=9.124615973910011e-07.\n",
      "reconstruction error=0.7916886885745293, variation=8.51862415829352e-07.\n",
      "reconstruction error=0.7916878934868706, variation=7.950876587115019e-07.\n",
      "reconstruction error=0.7916871515567268, variation=7.419301438771342e-07.\n",
      "reconstruction error=0.7916864593678855, variation=6.921888412447785e-07.\n",
      "reconstruction error=0.7916858136983644, variation=6.45669521071035e-07.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-3445fdac0593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparafac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "U, V, W = tensorly.decomposition.parafac(tensor=tensor_res, rank=5, verbose=True, return_errors=1, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, 4, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(U, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2308460"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res > 0) + np.sum(tensor_res == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2308460"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "44*5*10493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
