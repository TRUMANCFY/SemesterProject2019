{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wikiapi\n",
    "import wikipedia as wiki\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['computer', 'sport', 'politics', 'science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSplit(inputContent):\n",
    "    res = nltk.sent_tokenize(inputContent)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "for tp in topics:\n",
    "    fileNames = os.listdir(os.path.join('data/', tp))\n",
    "    for fileName in fileNames:\n",
    "        with open(os.path.join(os.path.join('data', tp), fileName)) as f:\n",
    "            text_dict[fileName] = sentenceSplit(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceRemoveStopwords(inputSentence):\n",
    "    \"\"\"\n",
    "        input: the string of input\n",
    "        output: list of stemmed tokens\n",
    "    \"\"\"\n",
    "    # lowercase\n",
    "    inputLower = inputSentence.lower()\n",
    "    # get the word and remove the punctuation\n",
    "    regTokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_filter = [t for t in regTokenizer.tokenize(inputLower) if t not in stop_words]\n",
    "    # stem the word\n",
    "    tokens_stemmer = [stemmer.stem(t) for t in tokens_filter]\n",
    "    return tokens_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens = defaultdict(list)\n",
    "for fileName, text in text_dict.items():\n",
    "    for sen in text:\n",
    "        wordTokens[fileName].append(sentenceRemoveStopwords(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word embedding\n",
    "allSents = []\n",
    "for s in wordTokens.values():\n",
    "    allSents = allSents + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(allSents, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '0ad' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-e8be9ba43eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0ad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comput'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \"\"\"\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '0ad' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.similarity('0ad', 'comput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2562"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sf',\n",
       " 'specul',\n",
       " 'literari',\n",
       " 'comic',\n",
       " 'hugo',\n",
       " 'amaz',\n",
       " 'feminist',\n",
       " 'horror',\n",
       " 'stargat',\n",
       " 'supernatur',\n",
       " 'wonder',\n",
       " 'fandom',\n",
       " 'fanzin',\n",
       " 'ptolemi',\n",
       " 'bachelor',\n",
       " 'politic',\n",
       " 'comt',\n",
       " 'durkheim',\n",
       " 'weber',\n",
       " 'marxist']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_complete = [[word for sent in sents for word in sent] for sents in wordTokens.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word = [dictionary.doc2bow(text) for text in text_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfModel = TfidfModel(dictionary=dictionary, corpus=bag_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfValue = tfidfModel[dictionary.doc2bow(text_complete[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_doc = len(text_complete)\n",
    "num_of_topics = len(topics)\n",
    "num_of_terms = len(tfidfModel.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_res = np.zeros((num_of_doc, num_of_topics, num_of_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_stem = ['comput', 'sport', 'politic', 'scienc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caifengyu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "topics_sim = np.zeros((num_of_terms, num_of_topics))\n",
    "for idx_term in range(num_of_terms):\n",
    "    for idx_topic in range(num_of_topics):\n",
    "        try:\n",
    "            topics_sim[idx_term, idx_topic] = model.similarity(topics_stem[idx_topic], tfidfModel.id2word[idx_term])\n",
    "        except:\n",
    "            topics_sim[idx_term, idx_topic] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35652"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_topics * num_of_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTFIDF(idxList):\n",
    "    res = np.zeros((num_of_terms))\n",
    "    for idx, val in idxList:\n",
    "        res[idx] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_text in range(num_of_doc):\n",
    "    tfidfValue = tfidfModel[dictionary.doc2bow(text_complete[idx_text])]\n",
    "    tfidfList = fillTFIDF(tfidfValue)\n",
    "    for idx_topic in range(num_of_topics):\n",
    "        tensor_res[idx_text, idx_topic, :] = topics_sim[:,idx_topic] * tfidfList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_res[tensor_res < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54137"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8913"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('tensor_0_7.mat', {'t_new': tensor_res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8068208195512172"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id['comput']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfModel.id2word[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('tensor_we.mat', {'t_new': tensor_res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Analog computer.txt', 'Computer graphics (computer science).txt', 'Computer graphics.txt', 'Computer hardware.txt', 'Computer network.txt', 'Computer program.txt', 'Computer science.txt', 'Computer security.txt', 'Computer.txt', 'Computing.txt', 'Cap (sport).txt', 'Combat sport.txt', 'Rai Sport.txt', 'Sport (botany).txt', 'Sport (US magazine).txt', 'Sport Chek.txt', 'Sport Express.txt', 'Sport psychology.txt', 'Sport.txt', 'Squash (sport).txt', 'Gridlock (politics).txt', 'Political economy.txt', 'Political party.txt', 'Political science.txt', 'Political spectrum.txt', 'Politics.txt', 'PS â€“ Political Science & Politics.txt', 'Branches of science.txt', 'Forensic science.txt', 'Natural science.txt', 'Science fiction.txt', 'Science.txt', 'Social science.txt'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1:\n",
    "    1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5:\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10:\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     4\n",
    "     3\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20:\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     3\n",
    "     1\n",
    "     1\n",
    "     3\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     4\n",
    "     3\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50:\n",
    "    1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     1\n",
    "     3\n",
    "     1\n",
    "     3\n",
    "     1\n",
    "     1\n",
    "     3\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     2\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     3\n",
    "     4\n",
    "     3\n",
    "     4\n",
    "     4\n",
    "     4\n",
    "     3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
