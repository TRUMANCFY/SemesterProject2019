{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-document Summarization using Tensor Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Introduction\n",
    "This notebook is a recurrance of the method of test labelling mentioned in the paper [http://www.scielo.org.mx/pdf/cys/v18n3/v18n3a12.pdf]\n",
    "\n",
    "There is mainly following steps mentioned in this notebookï¼š\n",
    "- data extraction and pre-processing\n",
    "- tensor construction\n",
    "- tensor decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. data extraction and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['computer', 'sport', 'politics', 'entertainment', 'science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('data')\n",
    "for t in topics:\n",
    "    os.makedirs(os.path.join('data', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wikiapi\n",
    "import wikipedia as wiki\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api = wikiapi.Wikipedia('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment\n",
      "science\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "for t in topics:\n",
    "    print(t)\n",
    "    tag = True\n",
    "    while tag:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            topic_dict[t] = wiki.search(t)\n",
    "            tag = False\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entertainment\n",
      "Entertainment!\n",
      "YG Entertainment\n",
      "JYP Entertainment\n",
      "SM Entertainment\n",
      "WWE\n",
      "Yuehua Entertainment\n",
      "Coridel Entertainment\n",
      "Sony Entertainment\n",
      "InXile Entertainment\n",
      "Science\n",
      "Science (disambiguation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caifengyu/anaconda3/lib/python3.6/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/caifengyu/anaconda3/lib/python3.6/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural science\n",
      "Science fiction\n",
      "Social science\n",
      "Forensic science\n",
      "Political science\n",
      "Rocket science\n",
      "Branches of science\n",
      "Computer science\n"
     ]
    }
   ],
   "source": [
    "for tk, tv in topic_dict.items():\n",
    "    for pageName in tv:\n",
    "        print(pageName)\n",
    "        tag = True\n",
    "        cnt = 0\n",
    "        while tag:\n",
    "            try:\n",
    "                pageTmp = wiki.page(pageName)\n",
    "                pageContent = pageTmp.content\n",
    "                with open(os.path.join(os.path.join('data', tk), pageName + '.txt'), 'w') as w:\n",
    "                    w.write(pageContent)\n",
    "                tag = False\n",
    "            except:\n",
    "                cnt += 1\n",
    "                if cnt < 10:\n",
    "                    continue\n",
    "                else:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will preprocess our data:\n",
    "- split the sentence\n",
    "- remove the stopwords\n",
    "- word stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Split the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSplit(inputContent):\n",
    "    res = nltk.sent_tokenize(inputContent)\n",
    "    res_filter = [i for i in res if len(i) > 10]\n",
    "    return res_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "for tp in topics:\n",
    "    fileNames = os.listdir(os.path.join('data/', tp))\n",
    "    for fileName in fileNames:\n",
    "        with open(os.path.join(os.path.join('data', tp), fileName)) as f:\n",
    "            text_dict[fileName] = sentenceSplit(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Remove the stop words and word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceRemoveStopwords(inputSentence):\n",
    "    \"\"\"\n",
    "        input: the string of input\n",
    "        outputL list of stemmed tokens\n",
    "    \"\"\"\n",
    "    # lowercase\n",
    "    inputLower = inputSentence.lower()\n",
    "    # get the word and remove the punctuation\n",
    "    regTokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_filter = [t for t in regTokenizer.tokenize(inputLower) if t not in stop_words]\n",
    "    # stem the word\n",
    "    tokens_stemmer = [stemmer.stem(t) for t in tokens_filter]\n",
    "    return tokens_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens = defaultdict(list)\n",
    "for fileName, text in text_dict.items():\n",
    "    for sen in text:\n",
    "        wordTokens[fileName].append(sentenceRemoveStopwords(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Topic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the complete textual data set\n",
    "text_complete = [[word for sent in sents for word in sent] for sents in wordTokens.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word = [dictionary.doc2bow(text) for text in text_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPIC = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=bag_of_word, num_topics=NUM_TOPIC, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.021*\"entertain\" + 0.016*\"sm\" + 0.010*\"yg\" + 0.010*\"compani\" + 0.007*\"music\"')\n",
      "(1, '0.018*\"fiction\" + 0.016*\"marvel\" + 0.016*\"scienc\" + 0.010*\"jyp\" + 0.009*\"entertain\"')\n",
      "(2, '0.035*\"comput\" + 0.013*\"use\" + 0.011*\"network\" + 0.009*\"system\" + 0.008*\"program\"')\n",
      "(3, '0.028*\"scienc\" + 0.019*\"polit\" + 0.010*\"social\" + 0.010*\"parti\" + 0.009*\"studi\"')\n",
      "(4, '0.018*\"sport\" + 0.016*\"entertain\" + 0.008*\"perform\" + 0.007*\"use\" + 0.006*\"wwe\"')\n"
     ]
    }
   ],
   "source": [
    "topic_select = ldamodel.print_topics(num_words=5)\n",
    "for tp in topic_select:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_complete = [sent for sents in wordTokens.values() for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_sent = corpora.Dictionary(sent_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_sent = [dictionary_sent.doc2bow(text) for text in sent_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPIC = 5\n",
    "ldamodel_sent = gensim.models.ldamodel.LdaModel(corpus=bag_of_word_sent, num_topics=NUM_TOPIC, id2word=dictionary_sent, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"sport\" + 0.014*\"parti\" + 0.010*\"state\" + 0.007*\"play\" + 0.007*\"world\"'),\n",
       " (1,\n",
       "  '0.053*\"scienc\" + 0.018*\"social\" + 0.015*\"natur\" + 0.015*\"polit\" + 0.012*\"studi\"'),\n",
       " (2,\n",
       "  '0.020*\"comput\" + 0.018*\"use\" + 0.012*\"system\" + 0.009*\"network\" + 0.006*\"inform\"'),\n",
       " (3,\n",
       "  '0.015*\"fiction\" + 0.015*\"entertain\" + 0.011*\"comput\" + 0.009*\"first\" + 0.009*\"new\"'),\n",
       " (4,\n",
       "  '0.016*\"wwe\" + 0.009*\"sport\" + 0.009*\"perform\" + 0.007*\"one\" + 0.006*\"event\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_select_sent = ldamodel_sent.print_topics(num_words=5)\n",
    "topic_select_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Tensor Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 build up the topic-term mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10493"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_terms = len(dictionary.id2token.keys())\n",
    "num_of_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_topics = len(topic_select_sent)\n",
    "num_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_doc = len(text_complete)\n",
    "num_of_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the map between topics and terms\n",
    "topic_term_mapping = np.zeros((num_of_terms, num_of_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_tp = 0.5\n",
    "for sent in sent_complete:\n",
    "    word_inds = dictionary_sent.doc2bow(sent)\n",
    "    topic_prob = ldamodel_sent.get_document_topics(word_inds)\n",
    "    for tp_prob in topic_prob:\n",
    "        if tp_prob[1] > thresh_tp:\n",
    "            for word_ind in word_inds:\n",
    "                topic_term_mapping[word_ind[0], tp_prob[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52465"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the total number of entry in topic_term_mapping\n",
    "num_of_terms * num_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16921.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the valid entry of topic_term_mapping\n",
    "np.sum(topic_term_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 build up tf-idf mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfModel = TfidfModel(dictionary=dictionary, corpus=bag_of_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 passage index to sentence index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_ind_to_sent_ind = lambda x: dictionary_sent.token2id[dictionary.id2token[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ind_to_passage_ind = lambda x: dictionary.token2id[dictionary_sent.id2token[x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 tensor construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTFIDF(idxList):\n",
    "    res = np.zeros((num_of_terms))\n",
    "    for idx, val in idxList:\n",
    "        res[idx] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_res = np.zeros((num_of_topics, num_of_doc, num_of_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_mapping_transfer = np.zeros((num_of_terms, num_of_topics))\n",
    "for idx_term in range(num_of_terms):\n",
    "    transfered_ind = sent_ind_to_passage_ind(idx_term)\n",
    "    for idx_topic in range(num_of_topics):\n",
    "        if topic_term_mapping[transfered_ind, idx_topic]:\n",
    "            topic_term_mapping_transfer[idx_term, idx_topic] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_text in range(len(text_complete)):\n",
    "    tfidfValue = tfidfModel[dictionary.doc2bow(text_complete[idx_text])]\n",
    "    tfidfList = fillTFIDF(tfidfValue)\n",
    "    for idx_topic in range(len(topic_select_sent)):\n",
    "        tensor_res[idx_topic, idx_text, :] = topic_term_mapping_transfer[:, idx_topic] * tfidfList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Tensor Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.decomposition import tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 44, 10493)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81655"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tensor.pickle', 'wb') as f:\n",
    "    pickle.dump(tensor_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorly.decomposition import tucker\n",
    "with open('tensor.pickle', 'rb') as f:\n",
    "    tensor_res = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 44, 10493)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = np.sum(tensor_res, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 44)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 0, 4, 0, 4, 3, 4, 0, 1, 1, 0, 1, 1,\n",
       "       0, 2, 2, 2, 1, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 3, 2, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(topic_terms, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_result = tucker(tensor_res, ranks=[44,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(decompose_result[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decompose_result[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10493, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decompose_result[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_res = np.reshape(decompose_result[1][0], (5,))\n",
    "doc_res = np.reshape(decompose_result[1][1], (44,))\n",
    "term_res = np.reshape(decompose_result[1][2], (10493,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc_mapping = np.outer(doc_res, topic_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(doc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(topic_doc_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83284"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
