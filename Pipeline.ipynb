{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-document Summarization using Tensor Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Introduction\n",
    "This notebook is a recurrance of the method of test labelling mentioned in the paper [http://www.scielo.org.mx/pdf/cys/v18n3/v18n3a12.pdf]\n",
    "\n",
    "There is mainly following steps mentioned in this notebookï¼š\n",
    "- data extraction and pre-processing\n",
    "- tensor construction\n",
    "- tensor decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. data extraction and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['computer', 'sport', 'politics', 'entertainment', 'science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e88f580f7d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.mkdir('data')\n",
    "for t in topics:\n",
    "    os.makedirs(os.path.join('data', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wikiapi\n",
    "import wikipedia as wiki\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api = wikiapi.Wikipedia('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment\n",
      "science\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "for t in topics:\n",
    "    print(t)\n",
    "    tag = True\n",
    "    while tag:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            topic_dict[t] = wiki.search(t)\n",
    "            tag = False\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entertainment\n",
      "Entertainment!\n",
      "YG Entertainment\n",
      "JYP Entertainment\n",
      "SM Entertainment\n",
      "WWE\n",
      "Yuehua Entertainment\n",
      "Coridel Entertainment\n",
      "Sony Entertainment\n",
      "InXile Entertainment\n",
      "Science\n",
      "Science (disambiguation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caifengyu/anaconda3/lib/python3.6/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/caifengyu/anaconda3/lib/python3.6/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural science\n",
      "Science fiction\n",
      "Social science\n",
      "Forensic science\n",
      "Political science\n",
      "Rocket science\n",
      "Branches of science\n",
      "Computer science\n"
     ]
    }
   ],
   "source": [
    "for tk, tv in topic_dict.items():\n",
    "    for pageName in tv:\n",
    "        print(pageName)\n",
    "        tag = True\n",
    "        cnt = 0\n",
    "        while tag:\n",
    "            try:\n",
    "                pageTmp = wiki.page(pageName)\n",
    "                pageContent = pageTmp.content\n",
    "                with open(os.path.join(os.path.join('data', tk), pageName + '.txt'), 'w') as w:\n",
    "                    w.write(pageContent)\n",
    "                tag = False\n",
    "            except:\n",
    "                cnt += 1\n",
    "                if cnt < 10:\n",
    "                    continue\n",
    "                else:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will preprocess our data:\n",
    "- split the sentence\n",
    "- remove the stopwords\n",
    "- word stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Split the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSplit(inputContent):\n",
    "    res = nltk.sent_tokenize(inputContent)\n",
    "    res_filter = [i for i in res if len(i) > 10]\n",
    "    return res_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "for tp in topics:\n",
    "    fileNames = os.listdir(os.path.join('data/', tp))\n",
    "    for fileName in fileNames:\n",
    "        with open(os.path.join(os.path.join('data', tp), fileName)) as f:\n",
    "            text_dict[fileName] = sentenceSplit(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Remove the stop words and word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceRemoveStopwords(inputSentence):\n",
    "    \"\"\"\n",
    "        input: the string of input\n",
    "        output: list of stemmed tokens\n",
    "    \"\"\"\n",
    "    # lowercase\n",
    "    inputLower = inputSentence.lower()\n",
    "    # get the word and remove the punctuation\n",
    "    regTokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens_filter = [t for t in regTokenizer.tokenize(inputLower) if t not in stop_words]\n",
    "    # stem the word\n",
    "    tokens_stemmer = [stemmer.stem(t) for t in tokens_filter]\n",
    "    return tokens_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTokens = defaultdict(list)\n",
    "for fileName, text in text_dict.items():\n",
    "    for sen in text:\n",
    "        wordTokens[fileName].append(sentenceRemoveStopwords(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Topic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the complete textual data set\n",
    "text_complete = [[word for sent in sents for word in sent] for sents in wordTokens.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word = [dictionary.doc2bow(text) for text in text_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPIC = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=bag_of_word, num_topics=NUM_TOPIC, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.039*\"comput\" + 0.012*\"program\" + 0.011*\"system\" + 0.010*\"use\" + 0.009*\"secur\"')\n",
      "(1, '0.020*\"scienc\" + 0.012*\"polit\" + 0.011*\"entertain\" + 0.007*\"use\" + 0.006*\"social\"')\n",
      "(2, '0.027*\"entertain\" + 0.015*\"sm\" + 0.012*\"yg\" + 0.010*\"compani\" + 0.009*\"music\"')\n",
      "(3, '0.019*\"comput\" + 0.013*\"network\" + 0.011*\"use\" + 0.010*\"sport\" + 0.009*\"graphic\"')\n",
      "(4, '0.019*\"sport\" + 0.014*\"wwe\" + 0.010*\"marvel\" + 0.008*\"wrestl\" + 0.007*\"compani\"')\n"
     ]
    }
   ],
   "source": [
    "topic_select = ldamodel.print_topics(num_words=5)\n",
    "for tp in topic_select:\n",
    "    print(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_complete = [sent for sents in wordTokens.values() for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_sent = corpora.Dictionary(sent_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_sent = [dictionary_sent.doc2bow(text) for text in sent_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPIC = 5\n",
    "ldamodel_sent = gensim.models.ldamodel.LdaModel(corpus=bag_of_word_sent, num_topics=NUM_TOPIC, id2word=dictionary_sent, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"wwe\" + 0.011*\"compani\" + 0.010*\"marvel\" + 0.008*\"state\" + 0.008*\"award\"'),\n",
       " (1,\n",
       "  '0.013*\"use\" + 0.011*\"program\" + 0.009*\"system\" + 0.008*\"comput\" + 0.007*\"stori\"'),\n",
       " (2,\n",
       "  '0.050*\"scienc\" + 0.017*\"social\" + 0.014*\"natur\" + 0.013*\"polit\" + 0.011*\"studi\"'),\n",
       " (3,\n",
       "  '0.033*\"comput\" + 0.014*\"use\" + 0.012*\"network\" + 0.007*\"system\" + 0.007*\"data\"'),\n",
       " (4,\n",
       "  '0.029*\"entertain\" + 0.018*\"sport\" + 0.009*\"parti\" + 0.008*\"game\" + 0.007*\"form\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_select_sent = ldamodel_sent.print_topics(num_words=5)\n",
    "topic_select_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Tensor Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 build up the topic-term mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10493"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_terms = len(dictionary.id2token.keys())\n",
    "num_of_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_topics = len(topic_select_sent)\n",
    "num_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_doc = len(text_complete)\n",
    "num_of_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the map between topics and terms\n",
    "topic_term_mapping = np.zeros((num_of_terms, num_of_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1a15006048>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_tp = 0.7\n",
    "for sent in sent_complete:\n",
    "    word_inds = dictionary.doc2bow(sent)\n",
    "    topic_prob = ldamodel.get_document_topics(word_inds)\n",
    "    for tp_prob in topic_prob:\n",
    "        if tp_prob[1] > thresh_tp:\n",
    "            for word_ind in word_inds:\n",
    "                topic_term_mapping[word_ind[0], tp_prob[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52465"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the total number of entry in topic_term_mapping\n",
    "num_of_terms * num_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16738.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the valid entry of topic_term_mapping\n",
    "np.sum(topic_term_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 build up tf-idf mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfModel = TfidfModel(dictionary=dictionary, corpus=bag_of_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 passage index to sentence index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_ind_to_sent_ind = lambda x: dictionary_sent.token2id[dictionary.id2token[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ind_to_passage_ind = lambda x: dictionary.token2id[dictionary_sent.id2token[x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 tensor construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTFIDF(idxList):\n",
    "    res = np.zeros((num_of_terms))\n",
    "    for idx, val in idxList:\n",
    "        res[idx] = val\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_res = np.zeros((num_of_doc, num_of_topics, num_of_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_mapping_transfer = np.zeros((num_of_terms, num_of_topics))\n",
    "for idx_term in range(num_of_terms):\n",
    "    transfered_ind = sent_ind_to_passage_ind(idx_term)\n",
    "    for idx_topic in range(num_of_topics):\n",
    "        if topic_term_mapping[transfered_ind, idx_topic]:\n",
    "            topic_term_mapping_transfer[idx_term, idx_topic] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_text in range(len(text_complete)):\n",
    "    tfidfValue = tfidfModel[dictionary.doc2bow(text_complete[idx_text])]\n",
    "    tfidfList = fillTFIDF(tfidfValue)\n",
    "    for idx_topic in range(len(topic_select_sent)):\n",
    "        tensor_res[idx_text, idx_topic, :] = topic_term_mapping_transfer[:, idx_topic] * tfidfList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Tensor Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorly.decomposition import tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5, 10493)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53979"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tensor.pickle', 'wb') as f:\n",
    "    pickle.dump(tensor_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('tensor_0_7.mat', {'t_new': tensor_res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5, 10493)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "tensor_res = loadmat('tensor_0_7.mat')['t_new']\n",
    "np.shape(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorly.decomposition import tucker\n",
    "with open('tensor.pickle', 'rb') as f:\n",
    "    tensor_res = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 44, 10493)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = np.sum(tensor_res, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 44)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 0, 4, 0, 4, 3, 4, 0, 1, 1, 0, 1, 1,\n",
       "       0, 2, 2, 2, 1, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 3, 2, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(topic_terms, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_result = tucker(tensor_res, ranks=[44,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 44)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decompose_result[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decompose_result[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10493, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decompose_result[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_res = np.reshape(decompose_result[1][0], (5,))\n",
    "doc_res = np.reshape(decompose_result[1][1], (44,))\n",
    "term_res = np.reshape(decompose_result[1][2], (10493,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc_mapping = np.outer(doc_res, topic_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(doc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(topic_doc_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83284"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly\n",
    "tensorly.default_backend\n",
    "from tensorly import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "t = np.random.rand(10,15,10000)\n",
    "np.shape(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5, 10493)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tensor_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 5, 5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_slice = tensor_res[:,:,:5000]\n",
    "np.shape(tensor_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a, b = decomposition.parafac(tensor=tensor_slice, rank=8, verbose=2, return_errors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8269160706104022,\n",
       " 0.7956067438993916,\n",
       " 0.7927929380366638,\n",
       " 0.7921968471463594,\n",
       " 0.7919448381652281,\n",
       " 0.7918226727587331,\n",
       " 0.7917586136990278,\n",
       " 0.7917221958472505,\n",
       " 0.7916998464486079,\n",
       " 0.7916852265779694,\n",
       " 0.7916751775942977,\n",
       " 0.7916680073596144,\n",
       " 0.7916627442290567,\n",
       " 0.7916587954607478,\n",
       " 0.7916557808247259,\n",
       " 0.7916534462306697,\n",
       " 0.791651616164578,\n",
       " 0.7916501660989234,\n",
       " 0.7916490057522255,\n",
       " 0.7916480685259318,\n",
       " 0.7916473046053221,\n",
       " 0.7916466763139541,\n",
       " 0.7916461549006026,\n",
       " 0.7916457182648812,\n",
       " 0.7916453493154372,\n",
       " 0.7916450347653766,\n",
       " 0.7916447642367661,\n",
       " 0.7916445295878676,\n",
       " 0.7916443244035017,\n",
       " 0.7916441436064691,\n",
       " 0.7916439831597597,\n",
       " 0.7916438398374203,\n",
       " 0.7916437110476827,\n",
       " 0.7916435946960716,\n",
       " 0.7916434890792152,\n",
       " 0.7916433928023098,\n",
       " 0.7916433047148477,\n",
       " 0.7916432238604821,\n",
       " 0.7916431494378388,\n",
       " 0.7916430807698296,\n",
       " 0.7916430172795732,\n",
       " 0.7916429584714505,\n",
       " 0.7916429039161658,\n",
       " 0.7916428532389274,\n",
       " 0.7916428061100651,\n",
       " 0.7916427622375553,\n",
       " 0.7916427213610397,\n",
       " 0.7916426832470135,\n",
       " 0.7916426476849339,\n",
       " 0.7916426144840578,\n",
       " 0.7916425834708508,\n",
       " 0.7916425544868492,\n",
       " 0.7916425273868899,\n",
       " 0.7916425020376208,\n",
       " 0.791642478316248,\n",
       " 0.79164245610947,\n",
       " 0.7916424353125611,\n",
       " 0.7916424158285784,\n",
       " 0.7916423975676716,\n",
       " 0.7916423804464772,\n",
       " 0.7916423643875786,\n",
       " 0.7916423493190322,\n",
       " 0.791642335173934,\n",
       " 0.7916423218900372,\n",
       " 0.7916423094094003,\n",
       " 0.7916422976780723,\n",
       " 0.7916422866457982,\n",
       " 0.7916422762657578,\n",
       " 0.7916422664943203]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35033892e+00, -2.76424136e-03, -6.72076213e-03,\n",
       "         1.21484372e-02, -3.72876181e-03],\n",
       "       [-8.33736161e-01, -9.56467681e-03,  5.80754431e-04,\n",
       "         1.06436844e-02,  1.69393424e-01],\n",
       "       [-8.26719391e-01, -8.55019883e-03, -1.49648024e-02,\n",
       "         1.49716073e-02,  1.46198983e-01],\n",
       "       [-1.40566561e+00, -1.14267190e-03, -6.03530572e-03,\n",
       "        -1.33571272e-04,  1.46502609e-02],\n",
       "       [-4.44692191e-01, -4.82951944e-03, -1.30539812e-02,\n",
       "         1.36493010e-02, -4.53775982e-03],\n",
       "       [-1.19452963e+00, -7.48483564e-04, -6.35827992e-03,\n",
       "         8.27069745e-03,  2.06363773e-03],\n",
       "       [-1.86055978e+00, -1.34677592e-03,  3.85696681e-03,\n",
       "         5.70947877e-02,  1.76395618e-02],\n",
       "       [-5.43386115e-01, -1.18432419e-02, -2.26282954e-02,\n",
       "         1.98919801e-02, -7.00115946e-03],\n",
       "       [-1.70727079e+00, -5.98447440e-05, -5.86137995e-03,\n",
       "         1.26653390e-02,  3.56147924e-03],\n",
       "       [-1.62506377e+00,  7.06226709e-04, -3.13360445e-04,\n",
       "         1.78696843e-02,  1.12594201e-02],\n",
       "       [-9.78492990e-03, -1.54472569e-03, -7.62302198e-03,\n",
       "         4.40836809e-03,  4.95387515e-05],\n",
       "       [-2.27621103e-02, -9.10874391e-04, -1.43289287e-02,\n",
       "         1.27681216e-02, -9.97154733e-05],\n",
       "       [-6.26174822e-03, -7.83812897e-04, -4.10691963e-03,\n",
       "         2.58241870e-03,  3.46440634e-04],\n",
       "       [-5.51307053e-03, -5.10318815e-04, -3.95595603e-03,\n",
       "         4.97951365e-03, -2.20584706e-05],\n",
       "       [-1.51448640e-02, -1.10313225e-03, -1.16079118e-02,\n",
       "         1.54960659e-02,  1.48199759e-03],\n",
       "       [-1.70375787e-02,  1.31890027e-04, -3.62985925e-03,\n",
       "         4.78617888e-03, -3.53929936e-04],\n",
       "       [-4.48655664e-03, -9.85733596e-04, -5.58577468e-03,\n",
       "         4.73413449e-03, -2.64130974e-05],\n",
       "       [-4.45714967e-02, -5.14973251e-03, -1.23455685e-02,\n",
       "         3.77783997e-02,  9.60603196e-04],\n",
       "       [-3.46232577e-02, -9.85846502e-03, -2.97196665e-02,\n",
       "         2.26534360e-02,  3.30949950e-04],\n",
       "       [-2.80910543e-02, -2.43135662e-03, -1.09555779e-02,\n",
       "         7.25338776e-03,  1.73298415e-04],\n",
       "       [-1.81781401e-02, -6.90760896e-02, -1.20299520e-02,\n",
       "         4.28506326e-03, -1.75888460e-03],\n",
       "       [-2.71131709e-02, -2.15709313e-01, -5.65340390e-03,\n",
       "         4.01967532e-02, -4.70961072e-03],\n",
       "       [-3.19521752e-02, -1.14193005e-01, -4.37561214e-02,\n",
       "        -1.15399525e-02, -3.83524209e-03],\n",
       "       [-4.05229779e-02, -2.89200405e-01,  5.89474605e-05,\n",
       "         7.66790997e-02, -5.72991772e-03],\n",
       "       [-5.60521347e-02, -1.31193998e-01, -1.12486330e-02,\n",
       "         2.87784297e-02, -2.07263796e-03],\n",
       "       [-3.81457995e-02, -2.65509400e-01, -1.47668696e-02,\n",
       "         2.71736291e-02, -6.71415794e-03],\n",
       "       [-7.38163531e-03, -1.15106347e-01,  2.55935932e-03,\n",
       "         2.31575178e-02, -2.53587134e-03],\n",
       "       [-1.77746277e-03, -1.41945846e-04, -5.73207979e-02,\n",
       "         2.82723815e-04,  8.93895212e-05],\n",
       "       [-8.17948131e-02, -6.92805940e-03, -2.96590400e-01,\n",
       "         2.37240562e-02,  3.30298820e-03],\n",
       "       [-8.17948131e-02, -6.92805940e-03, -2.96590400e-01,\n",
       "         2.37240562e-02,  3.30298820e-03],\n",
       "       [-2.29069807e-02, -1.25386154e-03, -3.05871914e-02,\n",
       "         3.58889440e-03,  2.41519798e-03],\n",
       "       [-1.19806470e-02, -9.83977391e-04, -8.45065556e-02,\n",
       "         3.87787542e-03,  9.58950516e-04],\n",
       "       [-1.18363220e-02, -2.03430739e-03, -3.11264383e-02,\n",
       "         4.57443718e-03,  1.33793453e-03],\n",
       "       [-1.80253146e-02, -2.07249703e-03, -8.12386913e-02,\n",
       "         4.16132895e-03,  9.52041093e-04],\n",
       "       [-6.55433552e-03, -1.88902712e-04, -6.11992602e-02,\n",
       "         1.77697122e-03,  1.97030297e-03],\n",
       "       [-1.49729123e-02, -1.95234577e-03, -1.50742893e-02,\n",
       "         5.91422594e-03,  2.76833923e-04],\n",
       "       [-1.35973938e-02, -1.17858873e-03, -6.83205783e-02,\n",
       "         4.02545581e-03,  6.50667490e-04],\n",
       "       [-6.29200088e-03,  1.60505459e-04, -7.82713861e-02,\n",
       "         2.45466822e-04,  1.22652305e-04],\n",
       "       [-2.24502371e-01, -6.53143221e-03, -6.85426752e-04,\n",
       "         1.64608422e-01,  1.21496910e-03],\n",
       "       [-3.92214460e-02, -4.25221706e-03, -7.97622658e-03,\n",
       "         3.15143107e-02,  1.20685148e-03],\n",
       "       [-6.54002704e-02,  2.84459792e-03, -1.14989293e-02,\n",
       "         2.04655984e-01, -4.76699523e-04],\n",
       "       [-6.13459881e-02, -6.16659502e-03, -2.41426924e-02,\n",
       "         4.76338580e-02,  1.09242238e-03],\n",
       "       [-1.56149948e-01, -1.22636052e-02, -1.79994461e-02,\n",
       "         2.00316707e-01,  1.66276316e-03],\n",
       "       [-9.52326944e-02, -5.91301252e-02, -1.07796953e-02,\n",
       "         1.52719638e-01,  6.80254264e-04]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 4, 0, 4, 0, 0, 0, 0, 0, 4, 4, 3, 3,\n",
       "       3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2308460"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tensor_res > 0) + np.sum(tensor_res == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
